<!doctype html>
<html lang="en" class="">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="description" content="Pretained Image Recognition Models">
    <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    <title>Transformer
    </title>

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css">
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href=""
        href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
    <link rel="stylesheet" href="css/style.css">
    <!-- <script>__md_scope=new URL(".",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script> -->

</head>

<body>
    <header class="">
        <nav class="navbar navbar-expand-lg navbar-light bg txt-white fixed-top">
            <div class="container-fluid container">
                <div class="">
                    <label><i class="ri-bar-chart-grouped-line"></i></label>
                    <a class="navbar-brand txt-white" href="#">Int Elligence</a>
                </div>

                <div class="d-flex ">
                    <a class="navbar-brand txt-white" href="https://github.com/IntElligence0">Packages</a>
                </div>
            </div>
        </nav>

    </header>

    <section class="sec-margin-top">
        <div>
            <div class="container-fluid">
                <div class="row">

                    <div class="col-lg-3 col-md-3 col-sm-3">
                        <div class="margin-top">
                            <h5>Tensorflow Forcasting Models</h5>
                            <div id="myDIV">
                                <div class=" flex activation margin-top active" style="width: 35%;">
                                    <a class="activation" href="index.html"><i class="ri-home-gear-fill"></i>

                                        <p style="margin-top: -26px; margin-left: 18px;">Getting start</p>
                                    </a>
                                </div>

                                <div class=" flex activation active" style="width: 42%;">
                                    <a class="activation" href="model summary.html"><i class="ri-file-list-3-fill"></i>
                                        <p style="margin-top: -26px; margin-left: 18px;">Model summarize</p>
                                    </a>
                                </div>
                                <div class="dropdown" style="width: 416px;">
                                    <a class="dropdown-toggle" href="#" id="dropdownMenuLink" data-bs-toggle="dropdown"
                                        aria-expanded="false"><i class="ri-article-fill"></i>
                                        Model Pages
                                    </a>

                                    <ul class="dropdown-menu" aria-labelledby="dropdownMenuLink"
                                        style="overflow: scroll; height: 357px;">
                                        <li><a class="dropdown-item activation hov" href="TCN.html">Temporal
                                                Convolutional
                                                Networks
                                                TCN</a>
                                        </li>
                                        <li><a class="dropdown-item activation hov"
                                                href="transformer.html">Transformer</a></li>
                                        <li><a class="dropdown-item activation hov" href="performer.html">Performer</a>
                                        </li>
                                        <li><a class="dropdown-item activation hov" href="CNN-LSTM.html">CNN Long
                                                Short-Term Memory
                                                Networks (CNN-LSTM)</a></li>
                                        <li><a class="dropdown-item activation hov" href="Time distributed.html">Time
                                                Distributed</a></li>
                                        <li><a class="dropdown-item activation hov" href="Bi-lstm.html">Bi-directional
                                                long short
                                                term memory (Bi-LSTM)</a></li>
                                        <li><a class="dropdown-item activation hov" href="BiGru.html">Bi-directional
                                                Gated
                                                Recurrent Units (Bi-GRU)</a></li>
                                        <li><a class="dropdown-item activation hov" href="gru.html">Gated Recurrent
                                                Units
                                                (GRU)</a></li>
                                        <li><a class="dropdown-item activation hov" href="cnn.html">Convolutional Neural
                                                Network (CNN)</a></li>
                                        <li><a class="dropdown-item activation hov" href="lstm.html">Long Short-Term
                                                Memory
                                                networks (LSTM)</a></li>
                                        <li><a class="dropdown-item activation hov" href="deepar.html">DeepAR</a></li>
                                        <li><a class="dropdown-item activation hov" href="tlnn.html">Time Lagged
                                                Neural
                                                Network (TLNN)</a></li>
                                        <li><a class="dropdown-item activation hov" href="bert.html">Bidirectional
                                                Encoder Representations from Transformers
                                                (Bert)</a></li>
                                        <li><a class="dropdown-item activation hov" href="Seq2seq.html">Seq2Seq
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="fnn.html">feedforward neural
                                                network (FNN)
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="N-BEATS.html">N-BEATS
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="Autoformer.html">Autoformer
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="rbfn.html">Radial basis
                                                function network (RBFN)
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="mlp.html">mlp
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="convlstm.html">convlstm
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="informer.html">Informer
                                            </a></li>
                                    </ul>
                                </div>
                                <div>
                                    <div class=" flex activation margin-top active" style="width: 35%;">
                                        <a class="activation" href="result.html"><i class="ri-question-answer-fill"></i>

                                            <p style="margin-top: -26px; margin-left: 18px;">Results</p>
                                        </a>
                                    </div>
                                    <div class=" flex activation  active" style="width: 42%;">
                                        <a class="activation" href="index.html"><i class="ri-file-edit-fill"></i>

                                            <p style="margin-top: -26px; margin-left: 18px;">Recent changes</p>
                                        </a>
                                    </div>

                                </div>
                            </div>


                        </div>

                    </div>
                    <div class="col-lg-9 col-md-12 col-sm-9 body">

                        <main>
                            <h1 class="">Basic Transformer Overview</h1>
                            <div>
                                <p>
                                    The Transformer architecture follows an encoder-decoder structure but does not rely
                                    on recurrence and convolutions in
                                    order to generate an output.
                                </p>
                                <div class="bg-light-gray bg-size">
                                    <img src="img/attention_research_1-727x1024.webp">
                                </div>

                            </div>
                            <div>
                                <p>
                                    In a nutshell, the task of the encoder, on the left half of the Transformer
                                    architecture, is to map an input sequence to
                                    a sequence of continuous representations, which is then fed into a decoder.
                                </p>
                                <div class="bg-light-gray bg-size">
                                    <img src="img/transformer_1-727x1024.webp">
                                </div>
                            </div>
                            <div>
                                <h2 class="">encoder overview</h2>

                                <div>
                                    <p>The encoder consists of a stack of N = 6 identical layers, where each layer is
                                        composed of two sublayers:
                                    </p>
                                    <p>The first sublayer implements a multi-head self-attention mechanism. You have
                                        seen that the multi-head mechanism
                                        implements h heads that receive a (different) linearly projected version of the
                                        queries, keys, and values, each to produce
                                        h outputs in parallel that are then used to generate a final result.</p>
                                </div>
                                <div class="bg-light-gray bg-size">
                                    <img src="img\image-32.png">
                                </div>
                                <p>We can see that with a left zero-padding of 2 entries we can achieve the same output
                                    length while obeying the causality
                                    rule. In fact, without dilation, the number of zero-padding entries required for
                                    maintaining the input length is always
                                    equal to kernel_size–1.</p>
                                <p>The second sublayer is a fully connected feed-forward network consisting of two
                                    linear transformations with Rectified
                                    Linear Unit (ReLU) activation in between:</p>
                                <p class="center" style="width: 100%;">FFN(x)=ReLU(W1x+b1)W2+b2</p>
                                <p>The six layers of the Transformer encoder apply the same linear transformations to
                                    all the words in the input sequence,
                                    but each layer employs different weight (W1,W2) and bias (b1,b2) parameters to do
                                    so.</p>
                                <p>Furthermore, each of these two sublayers has a residual connection around it.</p>
                                <p>
                                    Each sublayer is also succeeded by a normalization layer,layernorm(.) , which
                                    normalizes the sum computed between the sublayer
                                    input,X , and the output generated by the sublayer itself,sublayer(X) :
                                </p>
                                <p class="center" style="width: 100%;">layernorm(X+sublayer(X))</p>
                                <p>An important consideration to keep in mind is that the Transformer architecture
                                    cannot inherently capture any
                                    information about the relative positions of the words in the sequence since it does
                                    not make use of recurrence. This
                                    information has to be injected by introducing positional encodings to the input
                                    embeddings.</p>
                                <p>The positional encoding vectors are of the same dimension as the input embeddings and
                                    are generated using sine and
                                    cosine functions of different frequencies. Then, they are simply summed to the input
                                    embeddings in order to inject the
                                    positional information.</p>
                            </div>
                            <div>
                                <h2>Transformer model</h2>
                                <p>now we will know how to use Transformer </p>
                                <h3>import package</h3>
                                <div class="" id="Install">
                                    <code class="flex" style="width:100%;">
                                        <p class="line "  id="myText" style="padding-left: 5px;">from <span class="import-color">Energy_Models</span> import 
                                        <span class="import-color">Transformer</span> as <span class="import-color">m</span>
                                        </p>
                                        <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                    </code>
                                </div>
                                <div>
                                    <h3>call the model</h3>
                                    <div class="" id="Install">
                                        <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;">model = m.<span class="import-color">Transformer(
                                                <span class="package-color">input_shape,n_outputs,head_size,num_heads,ff_dim,num_transformer_blocks,
                                                    dropout,mlp_units</span>)</span>.getModel()</p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                    </div>
                                    <h4>model's parameter</h4>
                                    <p><span class="package-color">input_shape</span> : the data that the model will use
                                        it for train
                                    </p>
                                    <p><span class="package-color">n_outputs</span> : number of coulmns that model
                                        will
                                        predict and by default its equal 1</p>
                                    <p><span class="package-color">head_size</span> : key_dim is Size of each attention
                                        head for query and key.</p>
                                    <p><span class="package-color">num_heads</span> Size of each attention head for
                                        query and key.</p>
                                    <p><span class="package-color">ff_dim</span> : number of filters for conventional
                                        layer</p>
                                    <p><span class="package-color">num_transformer_blocks</span> : number of transformer
                                        blocks
                                    </p>
                                    <p><span class="package-color">mlp_units</span> : number of hidden layer</p>
                                    <p><span class="package-color">dropout</span> : which helps prevent overfitting</p>
                                    <p><span class="package-color">mlp_units</span> : number of hidden layer</p>

                                </div>
                                <div>
                                    <h3>compile the model</h3>
                                    <div class="" id="Install">
                                        <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;">model.compile(<span class="import-color">optimizer='adam', loss ='mse'</span>)
                                            </p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                    </div>
                                    <h4>compile parameter </h4>
                                    <p><span class="package-color">optimizer : </span> While training the deep learning
                                        model, we need to modify each epoch’s weights and minimize the loss function. An
                                        optimizer is a function or an algorithm that modifies the attributes of the
                                        neural network, such as weights and learning
                                        rate. Thus, it helps in reducing the overall loss and improve the accuracy.
                                        <span class="mid_grey">here we use adam</span>
                                    </p>
                                    <p><span class="package-color">loss : </span>the Loss function is a method of
                                        evaluating how well your algorithm is modeling your dataset</p>
                                </div>
                                <div>
                                    <h3>fit the model</h3>
                                    <div class="" id="Install">
                                        <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;">history = model.fit(<span class="import-color">X_train, y_train, batch_size=256, validation_split=0.3,epochs=85, verbose=1,
                                            shuffle=False)</span>
                                            </p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                    </div>
                                    <h4>fit's parameter</h4>
                                    <p>frist you pass your data then </p>
                                    <p><span class="package-color">batch_size : </span>refers to the number of training
                                        examples utilized in one iteration</p>
                                    <p><span class="package-color">validation_split : </span>Float between 0 and 1.
                                        Fraction of the training data to be used as validation data; selected from the
                                        last samples in the x and y data provided, before shuffling.</p>
                                    <p><span class="package-color">epochs : </span>The number of epochs is a
                                        hyperparameter
                                        that defines the number times that the learning algorithm will work through the
                                        entire training dataset.</p>
                                    <p><span class="package-color">verbose : </span>By setting verbose 0, 1 or 2 you
                                        just
                                        say how do you want to 'see' the training progress for each epoch.</p>

                                </div>
                                <div>
                                    <h3>Evaluation_Metrix</h3>
                                    <h3>import package</h3>
                                    <div class="" id="Install">
                                        <code class="flex" style="width:100%;">
                                        <p class="line "  id="myText" style="padding-left: 5px;">from <span class="import-color">Energy_Models</span> import 
                                        <span class="import-color">Evaluation_Metrix</span> as <span>mx</span>
                                        </p>
                                        <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                    </code>
                                    </div>
                                    <div class="" id="Install">
                                        <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;">mx.print_metrics(<span class="import-color">y_train,y_pred_train,y_test
                                                ,y_pred_test</span>
                                                =m.predict((<span class="import-color">X_train</span>
                                            )</p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                    </div>
                                </div>
                                <h3>build it from scratch</h3>
                                <div class="" id="Install">
                                    <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;"> <a href="https://github.com/IntElligence0/Energy_apps_Tk/blob/main/models_from_scratch/Transformer.py" class=""><span class="import-color padding">THE SOURCE CODE <i style="float:right;" class="padding ri-code-box-fill"></i></span></a>
                                            </p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                </div>
                            </div>
                        </main>
                        <nav id="toc" class="Fixed">
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <footer class="bg text-center text-lg-start txt-white">
        <!-- Copyright -->
        <div class="text-center p-3" style="background-color: rgba(0, 0, 0, 0.2);">
            © 2020 Copyright:
            <a class="txt-white" href="">created by Int Elligence Team</a>
        </div>
        <!-- Copyright -->
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="js/script.js"></script>


</body>