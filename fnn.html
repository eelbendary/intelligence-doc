<!doctype html>
<html lang="en" class="">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="description" content="Pretained Image Recognition Models">
    <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    <title>DeepAR</title>

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css">
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href=""
        href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
    <link rel="stylesheet" href="css/style.css">
    <!-- <script>__md_scope=new URL(".",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script> -->

</head>

<body>
    <header class="">
        <nav class="navbar navbar-expand-lg navbar-light bg txt-white fixed-top">
            <div class="container-fluid container">
                <div class="">
                    <label><i class="ri-bar-chart-grouped-line"></i></label>
                    <a class="navbar-brand txt-white" href="#">Int Elligence</a>
                </div>

                <div class="d-flex ">
                    <a class="navbar-brand txt-white" href="https://github.com/IntElligence0">Packages</a>
                </div>
            </div>
        </nav>

    </header>

    <section class="sec-margin-top">
        <div>
            <div class="container-fluid">
                <div class="row">

                    <div class="col-lg-3 col-md-3 col-sm-3">
                        <div class="margin-top">
                            <h5>Tensorflow Forcasting Models</h5>
                            <div id="myDIV">
                                <div class=" flex activation margin-top active" style="width: 35%;">
                                    <a class="activation" href="index.html"><i class="ri-home-gear-fill"></i>

                                        <p style="margin-top: -26px; margin-left: 18px;">Getting start</p>
                                    </a>
                                </div>

                                <div class=" flex activation active" style="width: 42%;">
                                    <a class="activation" href="model summary.html"><i class="ri-file-list-3-fill"></i>
                                        <p style="margin-top: -26px; margin-left: 18px;">Model summarize</p>
                                    </a>
                                </div>
                                <div class="dropdown" style="width: 416px;">
                                    <a class="dropdown-toggle" href="#" id="dropdownMenuLink" data-bs-toggle="dropdown"
                                        aria-expanded="false"><i class="ri-article-fill"></i>
                                        Model Pages
                                    </a>

                                    <ul class="dropdown-menu" aria-labelledby="dropdownMenuLink"
                                        style="overflow: scroll; height: 357px;">
                                        <li><a class="dropdown-item activation hov" href="TCN.html">Temporal
                                                Convolutional
                                                Networks
                                                TCN</a>
                                        </li>
                                        <li><a class="dropdown-item activation hov"
                                                href="transformer.html">Transformer</a></li>
                                        <li><a class="dropdown-item activation hov" href="performer.html">Performer</a>
                                        </li>
                                        <li><a class="dropdown-item activation hov" href="CNN-LSTM.html">CNN Long
                                                Short-Term Memory
                                                Networks (CNN-LSTM)</a></li>
                                        <li><a class="dropdown-item activation hov" href="Time distributed.html">Time
                                                Distributed</a></li>
                                        <li><a class="dropdown-item activation hov" href="Bi-lstm.html">Bi-directional
                                                long short
                                                term memory (Bi-LSTM)</a></li>
                                        <li><a class="dropdown-item activation hov" href="BiGru.html">Bi-directional
                                                Gated
                                                Recurrent Units (Bi-GRU)</a></li>
                                        <li><a class="dropdown-item activation hov" href="gru.html">Gated Recurrent
                                                Units
                                                (GRU)</a></li>
                                        <li><a class="dropdown-item activation hov" href="cnn.html">Convolutional Neural
                                                Network (CNN)</a></li>
                                        <li><a class="dropdown-item activation hov" href="lstm.html">Long Short-Term
                                                Memory
                                                networks (LSTM)</a></li>
                                        <li><a class="dropdown-item activation hov" href="deepar.html">DeepAR</a></li>
                                        <li><a class="dropdown-item activation hov" href="tlnn.html">Time Lagged
                                                Neural
                                                Network (TLNN)</a></li>
                                        <li><a class="dropdown-item activation hov" href="bert.html">Bidirectional
                                                Encoder Representations from Transformers
                                                (Bert)</a></li>
                                        <li><a class="dropdown-item activation hov" href="Seq2seq.html">Seq2Seq
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="fnn.html">feedforward neural
                                                network (FNN)
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="N-BEATS.html">N-BEATS
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="Autoformer.html">Autoformer
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="rbfn.html">Radial basis
                                                function network (RBFN)
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="mlp.html">mlp
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="convlstm.html">convlstm
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="informer.html">Informer
                                            </a></li>
                                    </ul>
                                </div>
                                <div>
                                    <div class=" flex activation margin-top active" style="width: 35%;">
                                        <a class="activation" href="result.html"><i class="ri-question-answer-fill"></i>

                                            <p style="margin-top: -26px; margin-left: 18px;">Results</p>
                                        </a>
                                    </div>
                                    <div class=" flex activation  active" style="width: 42%;">
                                        <a class="activation" href="index.html"><i class="ri-file-edit-fill"></i>

                                            <p style="margin-top: -26px; margin-left: 18px;">Recent changes</p>
                                        </a>
                                    </div>

                                </div>
                            </div>


                        </div>

                    </div>
                    <div class="col-lg-9 col-md-12 col-sm-9 body">

                        <main>
                            <h1 class="">Basic feedforward neural
                                network (FNN) Overview
                                <p>
                                    A feedforward neural network (FNN) is an artificial neural network wherein
                                    connections between the nodes do not form a
                                    cycle.[1] As such, it is different from its descendant: recurrent neural networks.
                                </p>

                                <p>The feedforward neural network was the first and simplest type of artificial neural
                                    network devised.[2] In this network,
                                    the information moves in only one direction—forward—from the input nodes, through
                                    the hidden nodes (if any) and to the
                                    output nodes. There are no cycles or loops in the network.[1]
                                </p>


                                <h2>Single-layer perceptron</h2>
                                <p>
                                    The simplest kind of neural network is a single-layer perceptron network, which
                                    consists of a single layer of output
                                    nodes; the inputs are fed directly to the outputs via a series of weights. The sum
                                    of the products of the weights and
                                    the inputs is calculated in each node, and if the value is above some threshold
                                    (typically 0) the neuron fires and takes
                                    the activated value (typically 1); otherwise it takes the deactivated value
                                    (typically -1). Neurons with this kind of
                                    activation function are also called artificial neurons or linear threshold units. In
                                    the literature the term perceptron
                                    often refers to networks consisting of just one of these units. A similar neuron was
                                    described by Warren McCulloch and
                                    Walter Pitts in the 1940s
                                </p>
                                <p>A perceptron can be created using any values for the activated and deactivated states
                                    as long as the threshold value
                                    lies between the two.</p>

                                <p>Perceptrons can be trained by a simple learning algorithm that is usually called the
                                    delta rule. It calculates the
                                    errors between calculated output and sample output data, and uses this to create an
                                    adjustment to the weights, thus
                                    implementing a form of gradient descent.</p>
                                <p>Single-layer perceptrons are only capable of learning linearly separable patterns; in
                                    1969 in a famous monograph titled
                                    Perceptrons, Marvin Minsky and Seymour Papert showed that it was impossible for a
                                    single-layer perceptron network to
                                    learn an XOR function (nonetheless, it was known that multi-layer perceptrons are
                                    capable of producing any possible
                                    boolean function).</p>
                                <p>Although a single threshold unit is quite limited in its computational power, it has
                                    been shown that networks of
                                    parallel threshold units can approximate any continuous function from a compact
                                    interval of the real numbers into the
                                    interval [-1,1]. This result can be found in Peter Auer, Harald Burgsteiner and
                                    Wolfgang Maass "A learning rule for very
                                    simple universal approximators consisting of a single layer of perceptrons".</p>

                                <div class=" bg-size">
                                    <img src="img/Feed_forward_neural_net.gif">
                                </div>


                                <h2>Multi-layer perceptron</h2>
                                <p>This class of networks consists of multiple layers of computational units, usually
                                    interconnected in a feed-forward way.
                                    Each neuron in one layer has directed connections to the neurons of the subsequent
                                    layer. In many applications the units
                                    of these networks apply a sigmoid function as an activation function. However
                                    sigmoidal activation functions have very
                                    small derivative values outside a small range and do not work well in deep neural
                                    networks due to the vanishing gradient
                                    problem.</p>
                                <p>The universal approximation theorem for neural networks states that every continuous
                                    function that maps intervals of
                                    real numbers to some output interval of real numbers can be approximated arbitrarily
                                    closely by a multi-layer perceptron
                                    with just one hidden layer. This result holds for a wide range of activation
                                    functions, e.g. for the sigmoidal functions</p>
                                <p>Multi-layer networks use a variety of learning techniques, the most popular being
                                    back-propagation. Here, the output
                                    values are compared with the correct answer to compute the value of some predefined
                                    error-function. By various
                                    techniques, the error is then fed back through the network. Using this information,
                                    the algorithm adjusts the weights of
                                    each connection in order to reduce the value of the error function by some small
                                    amount. After repeating this process
                                    for a sufficiently large number of training cycles, the network will usually
                                    converge to some state where the error of
                                    the calculations is small. In this case, one would say that the network has learned
                                    a certain target function. To adjust
                                    weights properly, one applies a general method for non-linear optimization that is
                                    called gradient descent. For this,
                                    the network calculates the derivative of the error function with respect to the
                                    network weights, and changes the weights
                                    such that the error decreases (thus going downhill on the surface of the error
                                    function). For this reason,
                                    back-propagation can only be applied on networks with differentiable activation
                                    functions.</p>
                                <p>In general, the problem of teaching a network to perform well, even on samples that
                                    were not used as training samples,
                                    is a quite subtle issue that requires additional techniques. This is especially
                                    important for cases where only very
                                    limited numbers of training samples are available.[4] The danger is that the network
                                    overfits the training data and
                                    fails to capture the true statistical process generating the data. Computational
                                    learning theory is concerned with
                                    training classifiers on a limited amount of data. In the context of neural networks
                                    a simple heuristic, called early
                                    stopping, often ensures that the network will generalize well to examples not in the
                                    training set.</p>
                                <p>Other typical problems of the back-propagation algorithm are the speed of convergence
                                    and the possibility of ending up
                                    in a local minimum of the error function. Today, there are practical methods that
                                    make back-propagation in multi-layer
                                    perceptrons the tool of choice for many machine learning tasks.
                                </p>
                                <p>One also can use a series of independent neural networks moderated by some
                                    intermediary, a similar behavior that happens
                                    in brain. These neurons can perform separably and handle a large task, and the
                                    results can be finally combined
                                </p>
                                <h2>Other feedforward networks</h2>
                                <p>More generally, any directed acyclic graph may be used for a feedforward network,
                                    with some nodes (with no parents)
                                    designated as inputs, and some nodes (with no children) designated as outputs. These
                                    can be viewed as multilayer
                                    networks where some edges skip layers, either counting layers backwards from the
                                    outputs or forwards from the inputs.
                                    Various activation functions can be used, and there can be relations between
                                    weights, as in convolutional neural
                                    networks.</p>
                                <p>Examples of other feedforward networks include radial basis function networks, which
                                    use a different activation function</p>
                                <p>Sometimes multi-layer perceptron is used loosely to refer to any feedforward neural
                                    network, while in other cases it is
                                    restricted to specific ones (e.g., with specific activation functions, or with fully
                                    connected layers, or trained by the
                                    perceptron algorithm).
                                </p>
                                <div>

                                    <h2>build Feed_forward_neural_network FNN model</h2>
                                    <p>now we will know how to use Feed_forward_neural_network FNN </p>
                                    <h3>import package</h3>
                                    <div class="" id="Install">
                                        <code class="flex" style="width:100%;">
                                        <p class="line "  id="myText" style="padding-left: 5px;">from <span class="import-color">Energy_Models</span> import 
                                        <span class="import-color">fnn</span> as <span class="import-color">m</span>
                                        </p>
                                        <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                    </code>
                                    </div>
                                    <div>
                                        <h3>call the model</h3>
                                        <div class="" id="Install">
                                            <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;">model = m.<span class="import-color">fnn(<span class="package-color">n_steps,n_features,n_outputs</span>)</span>.getModel()
                                            </p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                        </div>
                                        <h4>model's parameter</h4>
                                        <p><span class="package-color">n_steps</span> : number of days you want to
                                            predict
                                            on
                                        </p>
                                        <p><span class="package-color">n_features</span> : number of columns in data
                                            that
                                            will
                                            give to model</p>
                                        <p><span class="package-color">n_outputs</span> : number of coulmns that model
                                            will
                                            predict and by default its equal 1</p>
                                    </div>
                                    <div>
                                        <h3>compile the model</h3>
                                        <div class="" id="Install">
                                            <code class="flex" style="width:100%;">
                                            <p class="line " id="myText" style="padding-left: 5px;">model.compile(<span class="import-color">optimizer='adam', loss
                                                    ='mse'</span>)
                                            </p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                        </div>
                                        <h4>compile parameter </h4>
                                        <p><span class="package-color">optimizer : </span> While training the deep
                                            learning
                                            model, we need to modify each epoch’s weights and minimize the loss
                                            function. An
                                            optimizer is a function or an algorithm that modifies the attributes of the
                                            neural network, such as weights and learning
                                            rate. Thus, it helps in reducing the overall loss and improve the accuracy.
                                            <span class="mid_grey">here we use adam</span>
                                        </p>
                                        <p><span class="package-color">loss : </span>the Loss function is a method of
                                            evaluating how well your algorithm is modeling your dataset</p>
                                    </div>
                                    <div>
                                        <h3>fit the model</h3>
                                        <div class="" id="Install">
                                            <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;">history = model.fit(<span class="import-color">X_train, y_train, batch_size=256, validation_split=0.3,epochs=85, verbose=1,
                                            shuffle=False)</span>
                                            </p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                        </div>
                                        <h4>fit's parameter</h4>
                                        <p>frist you pass your data then </p>
                                        <p><span class="package-color">batch_size : </span>refers to the number of
                                            training
                                            examples utilized in one iteration</p>
                                        <p><span class="package-color">validation_split : </span>Float between 0 and 1.
                                            Fraction of the training data to be used as validation data; selected from
                                            the
                                            last samples in the x and y data provided, before shuffling.</p>
                                        <p><span class="package-color">epochs : </span>The number of epochs is a
                                            hyperparameter
                                            that defines the number times that the learning algorithm will work through
                                            the
                                            entire training dataset.</p>
                                        <p><span class="package-color">verbose : </span>By setting verbose 0, 1 or 2 you
                                            just
                                            say how do you want to 'see' the training progress for each epoch.</p>

                                    </div>
                                    <div>
                                        <h3>Evaluation_Metrix</h3>
                                        <h3>import package</h3>
                                        <div class="" id="Install">
                                            <code class="flex" style="width:100%;">
                                        <p class="line "  id="myText" style="padding-left: 5px;">from <span class="import-color">Energy_Models</span> import 
                                        <span class="import-color">Evaluation_Metrix</span> as <span>mx</span>
                                        </p>
                                        <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                    </code>
                                        </div>
                                        <div class="" id="Install">
                                            <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;">mx.print_metrics(<span class="import-color">y_train,y_pred_train,y_test
                                                ,y_pred_test</span>
                                                =m.predict((<span class="import-color">X_train</span>
                                            )</p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                        </div>
                                    </div>
                                    <div>


                                        <h3>predict the moodel for testing</h3>
                                        <div class="" id="Install">
                                            <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;"><span class="import-color">y_pred_test</span>
                                                =m.predict((<span class="import-color">X_test</span>
                                            )</p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                        </div>
                                    </div>
                                    <div>


                                        <h3>predict the moodel for training</h3>
                                        <div class="" id="Install">
                                            <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;"><span class="import-color">y_pred_test</span>
                                                =m.predict((<span class="import-color">X_train</span>
                                            )</p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                        </div>
                                    </div>
                                    <h3>build it from scratch</h3>
                                    <div class="" id="Install">
                                        <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;"> <a href="scratch/LSTM.py" class=""><span class="import-color padding">THE SOURCE CODE <i style="float:right;" class="padding ri-code-box-fill"></i></span></a>
                                            </p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                    </div>
                                </div>
                        </main>
                        <nav id="toc" class="Fixed">
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <footer class="bg text-center text-lg-start txt-white">
        <!-- Copyright -->
        <div class="text-center p-3" style="background-color: rgba(0, 0, 0, 0.2);">
            © 2020 Copyright:
            <a class="txt-white" href="">created by Int Elligence Team</a>
        </div>
        <!-- Copyright -->
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="js/script.js"></script>


</body>