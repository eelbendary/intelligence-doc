<!doctype html>
<html lang="en" class="">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="description" content="Pretained Image Recognition Models">
    <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    <title>Performer
    </title>

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css">
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href=""
        href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
    <link rel="stylesheet" href="css/style.css">
    <!-- <script>__md_scope=new URL(".",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script> -->

</head>

<body>
    <header class="">
        <nav class="navbar navbar-expand-lg navbar-light bg txt-white fixed-top">
            <div class="container-fluid container">
                <div class="">
                    <label><i class="ri-bar-chart-grouped-line"></i></label>
                    <a class="navbar-brand txt-white" href="#">Int Elligence</a>
                </div>

                <div class="d-flex ">
                    <a class="navbar-brand txt-white" href="https://github.com/IntElligence0">Packages</a>
                </div>
            </div>
        </nav>

    </header>

    <section class="sec-margin-top">
        <div>
            <div class="container-fluid">
                <div class="row">

                    <div class="col-lg-3 col-md-3 col-sm-3">
                        <div class="margin-top">
                            <h5>Tensorflow Forcasting Models</h5>
                            <div id="myDIV">
                                <div class=" flex activation margin-top active" style="width: 35%;">
                                    <a class="activation" href="index.html"><i class="ri-home-gear-fill"></i>

                                        <p style="margin-top: -26px; margin-left: 18px;">Getting start</p>
                                    </a>
                                </div>

                                <div class=" flex activation active" style="width: 42%;">
                                    <a class="activation" href="model summary.html"><i class="ri-file-list-3-fill"></i>
                                        <p style="margin-top: -26px; margin-left: 18px;">Model summarize</p>
                                    </a>
                                </div>
                                <div class="dropdown" style="width: 416px;">
                                    <a class="dropdown-toggle" href="#" id="dropdownMenuLink" data-bs-toggle="dropdown"
                                        aria-expanded="false"><i class="ri-article-fill"></i>
                                        Model Pages
                                    </a>

                                    <ul class="dropdown-menu" aria-labelledby="dropdownMenuLink"
                                        style="overflow: scroll; height: 357px;">
                                        <li><a class="dropdown-item activation hov" href="TCN.html">Temporal
                                                Convolutional
                                                Networks
                                                TCN</a>
                                        </li>
                                        <li><a class="dropdown-item activation hov"
                                                href="transformer.html">Transformer</a></li>
                                        <li><a class="dropdown-item activation hov" href="performer.html">Performer</a>
                                        </li>
                                        <li><a class="dropdown-item activation hov" href="CNN-LSTM.html">CNN Long
                                                Short-Term Memory
                                                Networks (CNN-LSTM)</a></li>
                                        <li><a class="dropdown-item activation hov" href="Time distributed.html">Time
                                                Distributed</a></li>
                                        <li><a class="dropdown-item activation hov" href="Bi-lstm.html">Bi-directional
                                                long short
                                                term memory (Bi-LSTM)</a></li>
                                        <li><a class="dropdown-item activation hov" href="BiGru.html">Bi-directional
                                                Gated
                                                Recurrent Units (Bi-GRU)</a></li>
                                        <li><a class="dropdown-item activation hov" href="gru.html">Gated Recurrent
                                                Units
                                                (GRU)</a></li>
                                        <li><a class="dropdown-item activation hov" href="cnn.html">Convolutional Neural
                                                Network (CNN)</a></li>
                                        <li><a class="dropdown-item activation hov" href="lstm.html">Long Short-Term
                                                Memory
                                                networks (LSTM)</a></li>
                                        <li><a class="dropdown-item activation hov" href="deepar.html">DeepAR</a></li>
                                        <li><a class="dropdown-item activation hov" href="tlnn.html">Time Lagged
                                                Neural
                                                Network (TLNN)</a></li>
                                        <li><a class="dropdown-item activation hov" href="bert.html">Bidirectional
                                                Encoder Representations from Transformers
                                                (Bert)</a></li>
                                        <li><a class="dropdown-item activation hov" href="Seq2seq.html">Seq2Seq
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="fnn.html">feedforward neural
                                                network (FNN)
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="N-BEATS.html">N-BEATS
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="Autoformer.html">Autoformer
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="rbfn.html">Radial basis
                                                function network (RBFN)
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="mlp.html">mlp
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="convlstm.html">convlstm
                                            </a></li>
                                        <li><a class="dropdown-item activation hov" href="informer.html">Informer
                                            </a></li>
                                    </ul>
                                </div>
                                <div>
                                    <div class=" flex activation margin-top active" style="width: 35%;">
                                        <a class="activation" href="result.html"><i class="ri-question-answer-fill"></i>

                                            <p style="margin-top: -26px; margin-left: 18px;">Results</p>
                                        </a>
                                    </div>
                                    <div class=" flex activation  active" style="width: 42%;">
                                        <a class="activation" href="index.html"><i class="ri-file-edit-fill"></i>

                                            <p style="margin-top: -26px; margin-left: 18px;">Recent changes</p>
                                        </a>
                                    </div>

                                </div>
                            </div>


                        </div>

                    </div>
                    <div class="col-lg-9 col-md-12 col-sm-9 body">

                        <main>
                            <h1 class="">Basic performer Overview</h1>
                            <div>
                                <p>
                                    An implementation of Performer, a linear attention-based transformer variant with a
                                    Fast Attention Via positive
                                    Orthogonal Random features approach (FAVOR+).
                                </p>
                                <div class="bg-light-gray bg-size">
                                    <img src="img/68747470733a2f2f696d6775722e636f6d2f616e61715853442e706e67.png">
                                </div>

                            </div>
                            <div>
                                <p>
                                    Transformer models have achieved state-of-the-art results across a diverse range of
                                    domains, including natural language,
                                    conversation, images, and even music. The core block of every Transformer
                                    architecture is the attention module, which
                                    computes similarity scores for all pairs of positions in an input sequence. This
                                    however, scales poorly with the length
                                    of the input sequence, requiring quadratic computation time to produce all
                                    similarity scores, as well as quadratic
                                    memory size to construct a matrix to store these scores.
                                </p>
                                <p>
                                    For applications where long-range attention is needed, several fast and more
                                    space-efficient proxies have been proposed
                                    such as memory caching techniques, but a far more common way is to rely on sparse
                                    attention. Sparse attention reduces
                                    computation time and the memory requirements of the attention mechanism by computing
                                    a limited selection of similarity
                                    scores from a sequence rather than all possible pairs, resulting in a sparse matrix
                                    rather than a full matrix. These
                                    sparse entries may be manually proposed, found via optimization methods, learned, or
                                    even randomized, as demonstrated by
                                    such methods as Sparse Transformers, Longformers, Routing Transformers, Reformers,
                                    and Big Bird. Since sparse matrices
                                    can also be represented by graphs and edges, sparsification methods are also
                                    motivated by the graph neural network
                                    literature, with specific relationships to attention outlined in Graph Attention
                                    Networks. Such sparsity-based
                                    architectures usually require additional layers to implicitly produce a full
                                    attention mechanism.

                                </p>
                                <div class="bg-light-gray bg-size">
                                    <img src="img/image12.jpg">
                                </div>
                            </div>
                            <div>
                                <h2 class="">Generalized Attention </h2>

                                <div>
                                    <p>In the original attention mechanism, the query and key inputs, corresponding
                                        respectively to rows and columns of a
                                        matrix, are multiplied together and passed through a softmax operation to form
                                        an attention matrix, which stores the
                                        similarity scores. Note that in this method, one cannot decompose the query-key
                                        product back into its original query and
                                        key components after passing it into the nonlinear softmax operation. However,
                                        it is possible to decompose the attention
                                        matrix back to a product of random nonlinear functions of the original queries
                                        and keys, otherwise known as random
                                        features, which allows one to encode the similarity information in a more
                                        efficient manner.
                                    </p>

                                </div>
                                <div class="bg-light-gray bg-size">
                                    <img src="img/image8.jpg">
                                </div>
                                <h2>Towards FAVOR+: Fast Attention via Matrix Associativity</h2>
                                <p>The decomposition described above allows one to store the implicit attention matrix
                                    with linear, rather than quadratic,
                                    memory complexity. One can also obtain a linear time attention mechanism using this
                                    decomposition. While the original
                                    attention mechanism multiplies the stored attention matrix with the value input to
                                    obtain the final result, after
                                    decomposing the attention matrix, one can rearrange matrix multiplications to
                                    approximate the result of the regular
                                    attention mechanism, without explicitly constructing the quadratic-sized attention
                                    matrix. This ultimately leads to
                                    FAVOR+.</p>
                                <div class="bg-light-gray bg-size">
                                    <img src="img/68747470733a2f2f696d6775722e636f6d2f616e61715853442e706e67.png">
                                </div>
                            </div>
                            <div>
                                <h2>performer model</h2>
                                <p>now we will know how to use performer </p>
                                <h3>import package</h3>
                                <div class="" id="Install">
                                    <code class="flex" style="width:100%;">
                                        <p class="line "  id="myText" style="padding-left: 5px;">from <span class="import-color">Energy_Models</span> import 
                                        <span class="import-color">Performer</span> as <span class="import-color">m</span>
                                        </p>
                                        <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                    </code>
                                </div>
                                <div>
                                    <h3>call the model</h3>
                                    <div class="" id="Install">
                                        <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;">model = m.<span class="import-color">performer(
                                                <span class="package-color">maxlen,n_features,n_outputs,vocab_size,embed_dim,num_heads,ff_dim,method
                                                    ,supports,rate</span>)</span>.getModel()</p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                    </div>

                                    <h4>model's parameter</h4>
                                    <p><span class="package-color">maxlen</span> : the data that the model will use
                                        it for train
                                    </p>
                                    <p><span class="package-color">n_features</span> : number of columns in data
                                        that
                                        will
                                        give to model</p>
                                    <p><span class="package-color">n_outputs</span> number of coulmns that model
                                        will
                                        predict and by default its equal 1</p>
                                    <p><span class="package-color">vocab_size</span> : number of coulmns that model will
                                        predict and by default its equal 1</p>
                                    <p><span class="package-color">embed_dim</span> : number of coulmns that model will
                                        predict and by default its equal 1</p>
                                    <p><span class="package-color">num_heads</span> Size of each attention head for
                                        query and key.</p>
                                    <p><span class="package-color">ff_dim</span> : number of filters for conventional
                                        layer</p>
                                    <p><span class="package-color">method</span> : which helps prevent overfitting</p>
                                    <p><span class="package-color">supports</span> : which helps prevent overfitting</p>
                                    <p><span class="package-color">rate</span> : number of coulmns that model will
                                        predict and by default its equal 1</p>

                                </div>
                                <div>
                                    <h3>compile the model</h3>
                                    <div class="" id="Install">
                                        <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;">model.compile(<span class="import-color">optimizer='adam', loss ='mse'</span>)
                                            </p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                    </div>
                                    <h4>compile parameter </h4>
                                    <p><span class="package-color">optimizer : </span> While training the deep learning
                                        model, we need to modify each epoch’s weights and minimize the loss function. An
                                        optimizer is a function or an algorithm that modifies the attributes of the
                                        neural network, such as weights and learning
                                        rate. Thus, it helps in reducing the overall loss and improve the accuracy.
                                        <span class="mid_grey">here we use adam</span>
                                    </p>
                                    <p><span class="package-color">loss : </span>the Loss function is a method of
                                        evaluating how well your algorithm is modeling your dataset</p>
                                </div>
                                <div>
                                    <h3>fit the model</h3>
                                    <div class="" id="Install">
                                        <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;">history = model.fit(<span class="import-color">X_train, y_train, batch_size=256, validation_split=0.3,epochs=85, verbose=1,
                                            shuffle=False)</span>
                                            </p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                    </div>
                                    <h4>fit's parameter</h4>
                                    <p>frist you pass your data then </p>
                                    <p><span class="package-color">batch_size : </span>refers to the number of training
                                        examples utilized in one iteration</p>
                                    <p><span class="package-color">validation_split : </span>Float between 0 and 1.
                                        Fraction of the training data to be used as validation data; selected from the
                                        last samples in the x and y data provided, before shuffling.</p>
                                    <p><span class="package-color">epochs : </span>The number of epochs is a
                                        hyperparameter
                                        that defines the number times that the learning algorithm will work through the
                                        entire training dataset.</p>
                                    <p><span class="package-color">verbose : </span>By setting verbose 0, 1 or 2 you
                                        just
                                        say how do you want to 'see' the training progress for each epoch.</p>

                                </div>
                                <div>

                                    <h3>evaluate the moodel for training</h3>
                                    <div class="" id="Install">
                                        <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;">m.evaluate(<span class="import-color">X_train, Y_train, verbose=1</span>
                                            )</p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                    </div>
                                </div>

                                <div>

                                    <h3>evaluate the moodel for testing</h3>
                                    <div class="" id="Install">
                                        <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;">m.evaluate(<span class="import-color">X_test, y_test, verbose=1</span>
                                            )</p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                    </div>
                                </div>
                                <h3>build it from scratch</h3>
                                <div class="" id="Install">
                                    <code class="flex" style="width:100%;">
                                            <p class="line "  id="myText" style="padding-left: 5px;"> <a href="https://github.com/IntElligence0/Energy_apps_Tk/blob/main/models_from_scratch/Performer.py" class=""><span class="import-color padding">THE SOURCE CODE <i style="float:right;" class="padding ri-code-box-fill"></i></span></a>
                                            </p>
                                            <!-- <button class="icon" onclick="copyContent()"><i class="ri-file-copy-line"></i></button> -->
                                        </code>
                                </div>
                            </div>
                        </main>
                        <nav id="toc" class="Fixed">
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <footer class="bg text-center text-lg-start txt-white">
        <!-- Copyright -->
        <div class="text-center p-3" style="background-color: rgba(0, 0, 0, 0.2);">
            © 2020 Copyright:
            <a class="txt-white" href="">created by Int Elligence Team</a>
        </div>
        <!-- Copyright -->
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="js/script.js"></script>


</body>